#!/usr/bin/env python

import subprocess,os,hashlib
import tempfile
import urlparse
import re
import argparse
import json

###########################################################################
###########################################################################

class http_headerblock(object):
    """
    HTTP header block, parsed from a pipe
    http_headerblock(pipe)

    Args:
        pipe (handle): an input file handle

    Attributes:
        http_ver (str): HTTP version token
        lines (list): list of header fields
    """

    def get_single_header(self,name):
        """Get a simple header, fail if multiple found"""
        try:
            allvalues = self.headers[name.upper()]
            if len(allvalues) != 1:
                return None
            return allvalues[0]
        except (KeyError):
            return None

    def get_duplicated_header(self,name):
        """Get a single header, if there are multiple headers, return
        only if they all match"""
        try:
            allvalues = self.headers[name.upper()]
            cl = allvalues[0]
            for s in allvalues[1:]:
                if s != cl:
                    return None
            return cl
        except KeyError:
            return None

    def get_content_length(self):
        n = self.get_duplicated_header('Content-Length')
        try:
            return int(n)
        except (ValueError,TypeError):
            return None

    def __str__(self):
        if self.http_ver:
            return "* [%s] %s %s %s\n"%(len(self.lines),self.http_ver,self.status_code,self.reason)
        else:
            return "?"

    def __init__(self, pipe):
        self.lines = None #: list of header lines
        self.headers = {}
        self.http_ver = None #: HTTP version token
        self.status_code = None #: HTTP status code (3 digit)
        self.reason = None #: HTTP reason phrase

        # https://tools.ietf.org/html/rfc7230#section-3.1.2
        # status-line = HTTP-version SP status-code SP reason-phrase CRLF
        l=''
        for l in pipe:
            if l: l=l.rstrip("\x20\x09\r\n")
            break
        else:
            return None

        (v,c,r)=l.split(None,2)
        (self_http_ver,self.status_code,self.reason)=(v,int(c),r.strip())

        # https://tools.ietf.org/html/rfc7230#section-3.1.2
        # header-field   = field-name ":" OWS field-value OWS
        lines=[]
        for l in pipe:
            if l: l=l.rstrip("\r\n")
            if l == '':
                break
            # XXX folding not tested
            if l[0] in "\0x20\0x09":
                l = l.strip("\x20\x09")
                if len(lines):
                    # not perfect for Set-Cookie, but that isn't in responses
                    lines[-1].append(" "+l)
                else:
                    # replicate bogus field
                    lines.append(" "+l)
            else:
                l = l.rstrip("\x20\x09")
                lines.append(l)

        # now parse the lines to header fields
        h={}
        for l in lines:
            try:
                (f,v)=l.split(':',1)
                # field-name shouldn't have any, but anyway
                f=f.strip("\x20\x09").upper()
                v=v.strip("\x20\x09")
                try:
                    h[f].append(v)
                except KeyError:
                    h[f]=[v]
            except ValueError:
                # XXX really bad, but stay robust
                pass

        self.lines = lines
        self.headers = h
        return

###########################################################################
###########################################################################

class singlefile(object):
    """
    Retrieve a single file by using curl and inspecting the output
    """

    def __init__(self,url,curlopts):
        self.error = "not initialized"
        self.orig_url = ""
        self.filename = None
        self.filesize = None
        self.hashes=[] # md5,sha1,sha256[,githash]
        self.server = None
        self.etag = None
        self.mimetype = None
        self.charset = None
        self.tmpname = None

        if '#' in url:
            ## TODO parse fragment for hash spec
            [url,frag] = url.split('#',1)
            url = url.split('#',1)[0]
        else:
            frag = None

        # save original url and extract possible filename (without parameters)
        self.orig_url = url
        parts = url.split('?',1)[0].split('/')
        if parts[-1] == '' or parts[-1][0] in '&?#' or parts[-1] == 'download':
            hunk=parts[-2]
        else:
            hunk=parts[-1]
        self.filename=hunk
        download_type='fetching'
        while True:
            print "-- %s %s"%(download_type,url)
            all_args = curlopts[:]
            all_args.append(url)
            run_curl = subprocess.Popen(all_args,stdout=subprocess.PIPE)

            hdr = http_headerblock(run_curl.stdout)
            # if via proxy, there might be a first HTTP response about the connection
            if None != hdr.lines and len(hdr.lines) == 0 and hdr.status_code == 200:
                hdr = http_headerblock(run_curl.stdout)

            if None == hdr.lines:
                res = run_curl.wait()
                if res == 35:
                    self.error = "curl returned err %d, try without -k"%(res,)
                elif res == 60:
                    self.error = "curl returned err %d, try with -k"%(res,)
                else:
                    self.error = "curl returned err %d"
                print "! %s"%self.error
                break

            ## check if redirect
            if (hdr.status_code / 100) == 3:
                self.error = "redirecting"
                run_curl.kill()
                # TODO handle failure
                loc = hdr.headers['LOCATION'][0]
                # unsure
                if ' ' in loc: loc = loc.replace(' ','%20')
                url = urlparse.urljoin(url,loc)
                download_type='redirected to'
                continue

            ## prepare to process body
            ## check status code to not give a chance to save error pages
            expected_size = hdr.get_content_length()
            print "CL",hdr.status_code,expected_size

            ## prepare hashing (git hash needs size beforehand)
            hash_md5=hashlib.md5()
            hash_sha1=hashlib.sha1()
            hash_sha256=hashlib.sha256()
            hash_githash=None
            if expected_size:
                hash_githash=hashlib.sha1()
                hash_githash.update(b"blob %d\0"%(expected_size))

            ## start downloading to a temporary file
            if hdr.status_code != 200:
                self.error = "HTTP status %d %s"%(hdr.status_code,hdr.reason)
                f = tempfile.mkstemp(prefix='.status%d'%(hdr.status),dir='.')
            else:
                self.error = "downloading"
                f = tempfile.mkstemp(prefix='.meta',dir='.')
            self.tmpname = f[1]; f = os.fdopen(f[0],"w")
            print self.tmpname,self.filename #XXX

            ## TODO progress
            counted_size = 0
            for buf in run_curl.stdout:
                f.write(buf)
                counted_size = counted_size + len(buf)
                hash_md5.update(buf)
                hash_sha1.update(buf)
                hash_sha256.update(buf)
                if hash_githash: hash_githash.update(buf)
            f.close()

            # check if download is complete
            if hdr.status_code == 200:
                statbuf = os.stat(self.tmpname)
                if statbuf.st_size != expected_size:
                    expected_size = statbuf.st_size
                if expected_size is None or counted_size == expected_size:
                    os.rename(self.tmpname,self.filename)
                    self.filesize = int(counted_size)
                    self.error = None
                elif expected_size is not None:
                    self.error = "loaded only %d bytes of %d"%(counted_size,expected_size)
                    os.remove(self.tmpname)
                    self.tmpname = None
                    break
            else:
                os.remove(self.tmpname)
                self.tmpname = None
                break

            self.server = hdr.get_single_header('Server')
            self.etag = hdr.get_single_header('Etag').strip("\x22\x27")
            mimearr = hdr.get_single_header('Content-Type').split(';')
            try:
                self.mimetype = mimearr[0].strip()
            except KeyError:
                self.mimetype = None
            for k in mimearr[1:]:
                if k.strip().upper()[0:8]=='CHARSET=':
                    # avoiding proper dequoting
                    self.charset = k[9:].strip("\x27\x22")

            ## TODO compare with expected hashes
            ## TODO save in db

            print "-- file:%s"%(self.filename,)
            print "-- size:%s"%(expected_size,)
            print "-- stamp:" # FIXME
            self.hashes=[hash_md5.hexdigest(),hash_sha1.hexdigest(),hash_sha256.hexdigest(),None]
            if hash_githash: self.hashes[3]=hash_githash.hexdigest()
            print "-- dig:%s"%self.hashes[0]
            print "-- sha1:%s"%self.hashes[1]
            print "-- sha256:%s"%self.hashes[2]
            print "-- githash:%s"%(self.hashes[3])
            print "-- mimetype:%s"%(self.mimetype)
            print "-- charset:%s"%(self.charset)
            # encoding
            # url
            # stored/present
            print "-- server:%s"%(self.server)
            print "-- etag:%s"%(self.etag)

            break
        return None

    def __del__(self):
        print "deleting",self.tmpname,self.filename
        try:
            if self.tmpname:
                os.remove(self.tmpname);
                print "removed %s"%(self.tmpname)
                self.tmpname = None
        except OSError: pass
        try:
            if self.filename:
                os.remove(self.filename)
                print "removed %s (debugging)"%(self.filename)
                self.filename = None
        except OSError: pass

###########################################################################
###########################################################################

def main():
    parser = argparse.ArgumentParser(description='Wrapper for curl to keep HTTP metadata',prog='metacurl')
## grep E\} src/tool_getparam.c|sed 's/[{} "]//g'
    # vvv
    ## Oa,remote-name-all,FALSE,
    ## r,range,TRUE,                                                                                                                                 R,remote-time,FALSE,
    ## s,silent,FALSE,                                                                                                                               S,show-error,FALSE,
    # i,include,FALSE,
    # k,insecure,FALSE,
    # K,config,TRUE,
    # H,header,TRUE,
    # Hp,proxy-header,TRUE,
    # D,dump-header,TRUE,
    parser.add_argument('-i','--include',action='store_true')
    parser.add_argument('--remote-name-all',action='store_true')
    parser.add_argument('-s','--silent',action='store_true')
    parser.add_argument('-r','--range')
    parser.add_argument('-k','--insecure',action='store_true')
    parser.add_argument('-K','--config')
    parser.add_argument('-H','--header',action='append')
    parser.add_argument('--data-binary',action='append')
    parser.add_argument('--proxy-header',action='store_true')
    parser.add_argument('--compressed',action='store_true')
    parser.add_argument('--socks5-hostname')
    parser.add_argument('-D','--dump-header')
    # ^^^
    # --2.0 ignored for compatibility
    # -s should be always there (silent)
    # -o should not be present (output)
    parser.add_argument('--2.0',action='store_const',const=False,default=False)
    # debug?
    parser.add_argument('URL',nargs='+')
    args = parser.parse_args()
    # XXX path for curl
    curl_args=["curl","-s","-i"]
    enc=json.JSONEncoder()

    print args
    for i,v in args.__dict__.items():
      if isinstance(v,list):
        for n in v:
          print '-- %s'%(i,),enc.encode(n)
      elif i in ['2_0','dump_header','silent','include']:
        if v:
          print "-- option %s stripped"%(i,)
        # silent and include will be added always
        pass
      elif v==True:
        curl_args.append('--%s'%(i,))
      elif v:
        curl_args.append('--%s=%s'%(i.replace('_','-'),v,))

    ## XXX add default headers (UA)
    curl_args.append('--')

    for url in args.URL:
        savedfile = singlefile(url,curl_args)
        print savedfile.__dict__

###########################################################################
###########################################################################

def curlopts():
	pass

###########################################################################
###########################################################################

if __name__ == '__main__':
  main()
